
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Compartmental Models &#8212; Learning Compartmental Models 1.0 documentation</title>
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Lcm API" href="../lcm%20API.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="compartmental-models">
<h1>Compartmental Models<a class="headerlink" href="#compartmental-models" title="Permalink to this headline">¶</a></h1>
<p>All the models inherits from CompartmentalModel an abstract class that extends pl.LightningModule.</p>
<div class="section" id="compartmentalmodel">
<h2>CompartmentalModel<a class="headerlink" href="#compartmentalmodel" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="lcm.compartmental_model.CompartmentalModel">
<em class="property">class </em><code class="sig-prename descclassname">lcm.compartmental_model.</code><code class="sig-name descname">CompartmentalModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compartmental Model abstract class from which other classes should extend</p>
<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.configure_optimizers">
<code class="sig-name descname">configure_optimizers</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.configure_optimizers" title="Permalink to this definition">¶</a></dt>
<dd><p>Choose what optimizers and learning-rate schedulers to use in your optimization.
Normally you’d need one. But in the case of GANs or similar you might have multiple.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><p>Any of these 6 options.</p>
<ul class="simple">
<li><p>Single optimizer.</p></li>
<li><p>List or Tuple - List of optimizers.</p></li>
<li><p>Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict).</p></li>
<li><p>Dictionary, with an ‘optimizer’ key, and (optionally) a ‘lr_scheduler’ key which value is a single LR scheduler or lr_dict.</p></li>
<li><p>Tuple of dictionaries as described, with an optional ‘frequency’ key.</p></li>
<li><p>None - Fit will run without any optimizer.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The ‘frequency’ value is an int corresponding to the number of sequential batches
optimized with the specific optimizer. It should be given to none or to all of the optimizers.
There is a difference between passing multiple optimizers in a list,
and passing multiple optimizers in dictionaries with a frequency of 1:
In the former case, all optimizers will operate on the given batch in each optimization step.
In the latter, only one optimizer will operate on the given batch at every step.</p>
<p>The lr_dict is a dictionary which contains scheduler and its associated configuration.
It has five keys. The default configuration is shown below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span> <span class="c1"># The LR schduler</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;epoch&#39;</span><span class="p">,</span> <span class="c1"># The unit of the scheduler&#39;s step size</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="c1"># The frequency of the scheduler</span>
    <span class="s1">&#39;reduce_on_plateau&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># For ReduceLROnPlateau scheduler</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="c1"># Metric for ReduceLROnPlateau to monitor</span>
    <span class="s1">&#39;strict&#39;</span><span class="p">:</span> <span class="kc">True</span> <span class="c1"># Whether to crash the training if `monitor` is not found</span>
<span class="p">}</span>
</pre></div>
</div>
<p>If user only provides LR schedulers, then their configuration will set to default as shown above.</p>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># most cases</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">opt</span>

<span class="c1"># multiple optimizer case (e.g.: GAN)</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span>

<span class="c1"># example with learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">generator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">disriminator_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">discriminator_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">generator_opt</span><span class="p">,</span> <span class="n">disriminator_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">discriminator_sched</span><span class="p">]</span>

<span class="c1"># example with step-based learning rate schedulers</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">gen_sched</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">ExponentialLR</span><span class="p">(</span><span class="n">gen_opt</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span>
                 <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">}</span>  <span class="c1"># called after each training step</span>
    <span class="n">dis_sched</span> <span class="o">=</span> <span class="n">CosineAnnealing</span><span class="p">(</span><span class="n">discriminator_opt</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># called every epoch</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">gen_opt</span><span class="p">,</span> <span class="n">dis_opt</span><span class="p">],</span> <span class="p">[</span><span class="n">gen_sched</span><span class="p">,</span> <span class="n">dis_sched</span><span class="p">]</span>

<span class="c1"># example with optimizer frequencies</span>
<span class="c1"># see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1</span>
<span class="c1"># https://arxiv.org/abs/1704.00028</span>
<span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">gen_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_gen</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
    <span class="n">dis_opt</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_disc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.02</span><span class="p">)</span>
    <span class="n">n_critic</span> <span class="o">=</span> <span class="mi">5</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">dis_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">n_critic</span><span class="p">},</span>
        <span class="p">{</span><span class="s1">&#39;optimizer&#39;</span><span class="p">:</span> <span class="n">gen_opt</span><span class="p">,</span> <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some things to know:</p>
<ul>
<li><p>Lightning calls <code class="docutils literal notranslate"><span class="pre">.backward()</span></code> and <code class="docutils literal notranslate"><span class="pre">.step()</span></code> on each optimizer
and learning rate scheduler as needed.</p></li>
<li><p>If you use 16-bit precision (<code class="docutils literal notranslate"><span class="pre">precision=16</span></code>), Lightning will automatically
handle the optimizers for you.</p></li>
<li><p>If you use multiple optimizers, <a class="reference internal" href="#lcm.compartmental_model.CompartmentalModel.training_step" title="lcm.compartmental_model.CompartmentalModel.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> will have an additional
<code class="docutils literal notranslate"><span class="pre">optimizer_idx</span></code> parameter.</p></li>
<li><p>If you use LBFGS Lightning handles the closure function automatically for you.</p></li>
<li><p>If you use multiple optimizers, gradients will be calculated only
for the parameters of current optimizer at each training step.</p></li>
<li><p>If you need to control how often those optimizers step or override the
default <code class="docutils literal notranslate"><span class="pre">.step()</span></code> schedule, override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code> hook.</p></li>
<li><p>If you only want to call a learning rate scheduler every <code class="docutils literal notranslate"><span class="pre">x</span></code> step or epoch,
or want to monitor a custom metric, you can specify these in a lr_dict:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scheduler&#39;</span><span class="p">:</span> <span class="n">lr_scheduler</span><span class="p">,</span>
    <span class="s1">&#39;interval&#39;</span><span class="p">:</span> <span class="s1">&#39;step&#39;</span><span class="p">,</span>  <span class="c1"># or &#39;epoch&#39;</span>
    <span class="s1">&#39;monitor&#39;</span><span class="p">:</span> <span class="s1">&#39;val_f1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;frequency&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.differential_equations">
<em class="property">abstract </em><code class="sig-name descname">differential_equations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.differential_equations" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of ODE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> – int, the time step.</p></li>
<li><p><strong>x</strong> – list, value of the state variables after previous step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>result of ODE at time t.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.get_rt">
<em class="property">abstract </em><code class="sig-name descname">get_rt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.get_rt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computation of basic reproduction number R_t at each
time step of a given time interval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>time_grid</strong> – A torch tensor of shape T with the time interval where to compute R_t.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor with R(t), with t in [0,…,T].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.integrate">
<code class="sig-name descname">integrate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.integrate" title="Permalink to this definition">¶</a></dt>
<dd><p>Integrate ODE on the given time interval time_grid.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>time_grid</strong> – time interval.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the solution of the ODE in time_grid.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.test_step">
<code class="sig-name descname">test_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_idx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.test_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the test set.
In this step you’d normally generate examples or calculate anything of interest
such as accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">test_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">test_batch</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">test_step</span><span class="p">(</span><span class="n">test_batch</span><span class="p">)</span>
    <span class="n">test_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">test_epoch_end</span><span class="p">(</span><span class="n">test_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple test datasets used).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><cite>None</cite> - Testing will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple validation datasets, <a class="reference internal" href="#lcm.compartmental_model.CompartmentalModel.test_step" title="lcm.compartmental_model.CompartmentalModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional
argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test datasets</span>
<span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#lcm.compartmental_model.CompartmentalModel.test_step" title="lcm.compartmental_model.CompartmentalModel.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.training_step">
<code class="sig-name descname">training_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_idx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.training_step" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> – </p></li>
<li><p><strong>batch_idx</strong> – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.compartmental_model.CompartmentalModel.validation_step">
<code class="sig-name descname">validation_step</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">batch</span></em>, <em class="sig-param"><span class="n">batch_idx</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.compartmental_model.CompartmentalModel.validation_step" title="Permalink to this definition">¶</a></dt>
<dd><p>Operates on a single batch of data from the validation set.
In this step you’d might generate examples or calculate anything of interest like accuracy.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># the pseudocode for these calls</span>
<span class="n">val_outs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val_batch</span> <span class="ow">in</span> <span class="n">val_data</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">(</span><span class="n">train_batch</span><span class="p">)</span>
    <span class="n">val_outs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">val_outs</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> | (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …) | [<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, …]) – The output of your <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code>. A tensor, tuple or list.</p></li>
<li><p><strong>batch_idx</strong> (<em>int</em>) – The index of this batch</p></li>
<li><p><strong>dataloader_idx</strong> (<em>int</em>) – The index of the dataloader that produced this batch
(only if multiple val datasets used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p>Any of.</p>
<blockquote>
<div><ul class="simple">
<li><p>Any object or value</p></li>
<li><p><cite>None</cite> - Validation will skip to the next batch</p></li>
</ul>
</div></blockquote>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># pseudocode of order</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_step</span><span class="p">()</span>
<span class="k">if</span> <span class="n">defined</span><span class="p">(</span><span class="s1">&#39;validation_step_end&#39;</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">validation_step_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">validation_epoch_end</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>

<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val datasets, validation_step will have an additional argument.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation datasets</span>
<span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you don’t need to validate you don’t need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#lcm.compartmental_model.CompartmentalModel.validation_step" title="lcm.compartmental_model.CompartmentalModel.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sir">
<h2>SIR<a class="headerlink" href="#sir" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="lcm.sir.SIR">
<em class="property">class </em><code class="sig-prename descclassname">lcm.sir.</code><code class="sig-name descname">SIR</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sir.SIR" title="Permalink to this definition">¶</a></dt>
<dd><p>SIR Compartmental Model</p>
<dl class="py method">
<dt id="lcm.sir.SIR.differential_equations">
<code class="sig-name descname">differential_equations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sir.SIR.differential_equations" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of ODE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> – int, the time step.</p></li>
<li><p><strong>x</strong> – list, value of the state variables after previous step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>result of ODE at time t.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.sir.SIR.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sir.SIR.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="lcm.sir.SIR.get_rt">
<code class="sig-name descname">get_rt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sir.SIR.get_rt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computation of basic reproduction number R_t at each
time step of a given time interval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>time_grid</strong> – A torch tensor of shape T with the time interval where to compute R_t.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor with R(t), with t in [0,…,T].</p>
</dd>
</dl>
<p>Code:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">r_t</span> <span class="o">=</span> <span class="n">extended_params</span><span class="p">[</span><span class="s1">&#39;beta&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">extended_params</span><span class="p">[</span><span class="s1">&#39;gamma&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="sidarthe">
<h2>SIDARTHE<a class="headerlink" href="#sidarthe" title="Permalink to this headline">¶</a></h2>
<a class="reference internal image-reference" href="../_images/sidarthe.png"><img alt="../_images/sidarthe.png" src="../_images/sidarthe.png" style="width: 500px;" /></a>
<dl class="py class">
<dt id="lcm.sidarthe.Sidarthe">
<em class="property">class </em><code class="sig-prename descclassname">lcm.sidarthe.</code><code class="sig-name descname">Sidarthe</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>SIDARTHE Compartmental Model:</dt><dd><p>Giordano, Giulia, et al.
“Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy.”
Nature Medicine (2020): 1-6.</p>
</dd>
</dl>
<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.cpu">
<code class="sig-name descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.
:returns: self
:rtype: Module</p>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.cuda">
<code class="sig-name descname">cuda</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.cuda" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU.
This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on GPU while being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> – if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.differential_equations">
<code class="sig-name descname">differential_equations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.differential_equations" title="Permalink to this definition">¶</a></dt>
<dd><p>Definition of ODE.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> – int, the time step.</p></li>
<li><p><strong>x</strong> – list, value of the state variables after previous step.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>result of ODE at time t.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Same as <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code>, however in Lightning you want this to define
the operations you want to use for prediction (i.e.: on a server or as a feature extractor).</p>
<p>Normally you’d call <code class="docutils literal notranslate"><span class="pre">self()</span></code> from your <code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code> method.
This makes it easy to write a complex system for training with the outputs
you’d want in a prediction setting.</p>
<p>You may also find the <code class="xref py py-func docutils literal notranslate"><span class="pre">auto_move_data()</span></code> decorator useful
when using the module outside Lightning in a production setting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> – Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong> – Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Predicted output</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># example if we were using this model as a feature extractor</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_maps</span>

<span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>

    <span class="c1"># ...</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="c1"># splitting it this way allows model to be used a feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModelAbove</span><span class="p">()</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">server</span><span class="o">.</span><span class="n">get_request</span><span class="p">()</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">server</span><span class="o">.</span><span class="n">write_results</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

<span class="c1"># -------------</span>
<span class="c1"># This is in stark contrast to torch.nn.Module where normally you would have this:</span>
<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
    <span class="n">feature_maps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">convnet</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.get_default_learning_rates">
<code class="sig-name descname">get_default_learning_rates</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.get_default_learning_rates" title="Permalink to this definition">¶</a></dt>
<dd><p>Setting all the params to the same learning rate value</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.get_rt">
<code class="sig-name descname">get_rt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">time_grid</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.get_rt" title="Permalink to this definition">¶</a></dt>
<dd><p>Computation of basic reproduction number R_t at each
time step of a given time interval.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>time_grid</strong> – A torch tensor of shape T with the time interval where to compute R_t.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor with R(t), with t in [0,…,T].</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="lcm.sidarthe.Sidarthe.to">
<code class="sig-name descname">to</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">device</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.sidarthe.Sidarthe.to" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves and/or casts the parameters and buffers.</p>
<p>This can be called as
.. function:: to(device=None, dtype=None, non_blocking=False)
.. function:: to(dtype, non_blocking=False)
.. function:: to(tensor, non_blocking=False)
Its signature is similar to <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.to()</span></code>, but only accepts
floating point desired <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code> s. In addition, this method will
only cast the floating point parameters and buffers to <code class="xref py py-attr docutils literal notranslate"><span class="pre">dtype</span></code>
(if given). The integral parameters and buffers will be moved
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code>, if that is given, but with dtypes unchanged. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">non_blocking</span></code> is set, it tries to convert/move asynchronously
with respect to the host if possible, e.g., moving CPU Tensors with
pinned memory to CUDA devices.
See below for examples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> – the desired device of the parameters
and buffers in this module</p></li>
<li><p><strong>dtype</strong> – the desired floating point type of
the floating point parameters and buffers in this module</p></li>
<li><p><strong>tensor</strong> – Tensor whose dtype and device are the desired
dtype and device for all parameters and buffers in this module</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">ExampleModule</span><span class="p">(</span><span class="n">DeviceDtypeModuleMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ExampleModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float64)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">half</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">cpu</span><span class="p">)</span>
<span class="go">ExampleModule()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">weight</span> 
<span class="go">tensor([[...]], dtype=torch.float16)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">device</span>
<span class="go">device(type=&#39;cpu&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">dtype</span>
<span class="go">torch.float16</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mobility-sidarthe">
<h2>Mobility SIDARTHE<a class="headerlink" href="#mobility-sidarthe" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="lcm.mobility_sidarthe.SidartheMobility">
<em class="property">class </em><code class="sig-prename descclassname">lcm.mobility_sidarthe.</code><code class="sig-name descname">SidartheMobility</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.mobility_sidarthe.SidartheMobility" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="lcm.mobility_sidarthe.SidartheMobility.differential_equations">
<code class="sig-name descname">differential_equations</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">t</span></em>, <em class="sig-param"><span class="n">x</span></em><span class="sig-paren">)</span><a class="headerlink" href="#lcm.mobility_sidarthe.SidartheMobility.differential_equations" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the right-hand side of SIDARTHE model enriched with Mobility</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> – time t at which right-hand side is computed</p></li>
<li><p><strong>x</strong> – (Tensor) state of model at time t forall the fitted areas. S x #states (=8). States are x[:,0] = S, x[:,1] = I, x[:,2] = D, x[:,3] = A, x[:,4] = R,  x[:,5] = T, x[:,6] = H, x[:,7] = E</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>right-hand side of Mobility SIDARTHE model, i.e. f(t,:,x(t)), the second dimension corresponds to the areas to fit.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="create-a-custom-model">
<h2>Create a Custom Model<a class="headerlink" href="#create-a-custom-model" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Learning Compartmental Models</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../Install%20and%20Import.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../lcm%20API.html">Lcm API</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Compartmental Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="trainers.html">Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="integrators.html">Integrators</a></li>
<li class="toctree-l2"><a class="reference internal" href="losses.html">Losses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../citing.html">Citing</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="../lcm%20API.html">Lcm API</a><ul>
      <li>Previous: <a href="../lcm%20API.html" title="previous chapter">Lcm API</a></li>
      <li>Next: <a href="datasets.html" title="next chapter">Datasets</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Andrea Zugarini, Enrico Meloni et al..
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.4.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/api/compartmental_models.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>