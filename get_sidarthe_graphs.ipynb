{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from learning_models.sidarthe import Sidarthe\n",
    "from populations import populations\n",
    "\n",
    "from utils.data_utils import select_data\n",
    "from utils.visualization_utils import generic_plot, Curve, format_xtick, generic_sub_plot, Plot\n",
    "\n",
    "from torch_euler import Heun\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load targets\n",
    "df_file = os.path.join(os.getcwd(), \"COVID-19\", \"dati-andamento-nazionale\", \"dpc-covid19-ita-andamento-nazionale.csv\")\n",
    "area = [\"ITA\"]\n",
    "area_col_name = \"stato\"  # \"Country/Region\"\n",
    "\n",
    "groupby_cols = [\"data\"]  # [\"Date\"]\n",
    "\n",
    "d_col_name = \"isolamento_domiciliare\"\n",
    "r_col_name = \"ricoverati_con_sintomi\"\n",
    "t_col_name = \"terapia_intensiva\"\n",
    "h_detected_col_name = \"dimessi_guariti\"\n",
    "e_col_name = \"deceduti\"  # \"Fatalities\"\n",
    "\n",
    "x_target, d_target = select_data(df_file, area, area_col_name, d_col_name, groupby_cols, file_sep=\",\")\n",
    "_, y_target = select_data(df_file, area, area_col_name, \"totale_positivi\", groupby_cols, file_sep=\",\")\n",
    "_, r_target = select_data(df_file, area, area_col_name, r_col_name, groupby_cols, file_sep=\",\")\n",
    "_, t_target = select_data(df_file, area, area_col_name, t_col_name, groupby_cols, file_sep=\",\")\n",
    "_, h_detected_target = select_data(df_file, area, area_col_name, h_detected_col_name, groupby_cols, file_sep=\",\")\n",
    "_, e_target = select_data(df_file, area, area_col_name, e_col_name, groupby_cols, file_sep=\",\")\n",
    "\n",
    "initial_len = len(y_target)\n",
    "tmp_d, tmp_r, tmp_t, tmp_h, tmp_e = [], [], [], [], []\n",
    "for i in range(initial_len):\n",
    "    if y_target[i] > 0:\n",
    "        tmp_d.append(d_target[i])\n",
    "        tmp_r.append(r_target[i])\n",
    "        tmp_t.append(t_target[i])\n",
    "        tmp_h.append(h_detected_target[i])\n",
    "        tmp_e.append(e_target[i])\n",
    "d_target = tmp_d\n",
    "r_target = tmp_r\n",
    "t_target = tmp_t\n",
    "h_detected_target = tmp_h\n",
    "e_target = tmp_e\n",
    "\n",
    "targets = {\n",
    "    \"d\": d_target,\n",
    "    \"r\": r_target,\n",
    "    \"t\": t_target,\n",
    "    \"h_detected\": h_detected_target,\n",
    "    \"e\": e_target\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load references\n",
    "references = {}\n",
    "param_keys = ['alpha', 'beta', 'gamma', 'delta', 'epsilon', 'theta', 'xi', 'eta', 'mu', 'nu', 'tau', 'lambda', 'kappa', 'zeta', 'rho', 'sigma']\n",
    "ref_df = pd.read_csv(os.path.join(os.getcwd(), \"regioni\", \"sidarthe_results.csv\"))\n",
    "for key in 'sidarthe':\n",
    "    references[key] = ref_df[key].tolist()\n",
    "\n",
    "for key in [\"r0\", \"h_detected\"]:\n",
    "    references[key] = ref_df[key].tolist()\n",
    "\n",
    "for key in param_keys:\n",
    "    references[key] = ref_df[key].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load experiment values\n",
    "exp_paths = os.path.join(os.getcwd(), \"regioni_sidarthe\")\n",
    "exp_id = \"52260e50-c7e4-45cb-b3b6-e11b1b51335a\"\n",
    "exp_path = os.path.join(exp_paths, exp_id)\n",
    "exp_settings_path = os.path.join(exp_path, \"settings.json\")\n",
    "exp_report_path = os.path.join(exp_path, \"final.json\")\n",
    "\n",
    "with open(exp_settings_path) as settings_json:\n",
    "    exp_settings = json.load(settings_json)\n",
    "\n",
    "with open(exp_report_path) as report_json:\n",
    "    exp_report = json.load(report_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# create trained model\n",
    "population = populations[\"Italy\"]\n",
    "integrator = Heun\n",
    "time_step = 1.\n",
    "\n",
    "params = exp_report[\"params\"]\n",
    "train_size = exp_settings[\"train_size\"]\n",
    "val_size = exp_settings[\"val_len\"]\n",
    "dataset_size = len(x_target)\n",
    "\n",
    "model_params = { \n",
    "    \"d_weight\": 1.,\n",
    "    \"r_weight\": 1.,\n",
    "    \"t_weight\": 1.,\n",
    "    \"h_weight\": 1.,\n",
    "    \"e_weight\": 1.,\n",
    "    \"der_1st_reg\": 0.,\n",
    "    \"bound_reg\": 0.,\n",
    "    \"verbose\": False,\n",
    "    \"loss_type\": \"rmse\",\n",
    "    \"references\": references,\n",
    "    \"targets\": targets,\n",
    "    \"train_size\": train_size,\n",
    "    \"val_size\": val_size\n",
    "}\n",
    "init_conditions_params = { \"population\": population }\n",
    "initial_conditions = Sidarthe.compute_initial_conditions_from_targets(targets, init_conditions_params)\n",
    "\n",
    "model = Sidarthe(params, population, initial_conditions, integrator, time_step, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute inference\n",
    "with torch.no_grad():\n",
    "    t_start = 0\n",
    "    t_end = train_size\n",
    "\n",
    "    t_grid = torch.linspace(0, 100, int(100 / time_step) + 1)\n",
    "    inferences = model.inference(t_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice dataset\n",
    "train_hat_slice = slice(t_start, int(train_size / time_step), int(1 / time_step))\n",
    "val_hat_slice = slice(int(train_size / time_step), int(train_size + val_size / time_step),int(1 / time_step))\n",
    "test_hat_slice = slice(int(train_size + val_size / time_step), int(dataset_size / time_step), int(1 / time_step))\n",
    "dataset_hat_slice = slice(t_start, int(dataset_size / time_step), int(1 / time_step))\n",
    "\n",
    "train_target_slice = slice(t_start, train_size, 1)\n",
    "val_target_slice = slice(train_size, train_size + val_size, 1)\n",
    "test_target_slice = slice(train_size + val_size, dataset_size, 1)\n",
    "dataset_target_slice = slice(t_start, dataset_size, 1)\n",
    "\n",
    "def slice_values(values, slice_):\n",
    "    return {key: value[slice_] for key, value in values.items()}\n",
    "\n",
    "hat_train = slice_values(inferences, train_hat_slice)\n",
    "hat_val = slice_values(inferences, val_hat_slice)\n",
    "hat_test = slice_values(inferences, test_hat_slice)\n",
    "hat_dataset = slice_values(inferences, dataset_hat_slice)\n",
    "\n",
    "target_train = slice_values(targets, train_target_slice)\n",
    "target_val = slice_values(targets, val_target_slice)\n",
    "target_test = slice_values(targets, test_target_slice)\n",
    "target_dataset = slice_values(targets, dataset_target_slice)\n",
    "\n",
    "\n",
    "references = { k: torch.tensor(v, dtype=model.dtype) for k,v in references.items() }\n",
    "\n",
    "references_train = slice_values(references, train_target_slice)\n",
    "references_val = slice_values(references, val_target_slice)\n",
    "references_test = slice_values(references, test_target_slice)\n",
    "references_dataset = slice_values(references, dataset_target_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute losses of our model\n",
    "our_train_risks = model.losses(\n",
    "    hat_train,\n",
    "    target_train\n",
    ")\n",
    "\n",
    "our_val_risks = model.losses(\n",
    "    hat_val,\n",
    "    target_val\n",
    ")\n",
    "\n",
    "our_test_risks = model.losses(\n",
    "    hat_test,\n",
    "    target_test\n",
    ")\n",
    "\n",
    "our_dataset_risks = model.losses(\n",
    "    hat_dataset,\n",
    "    target_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute losses of nature sidarthe model\n",
    "nature_train_risks = model.losses(\n",
    "    references_train,\n",
    "    target_train\n",
    ")\n",
    "\n",
    "nature_val_risks = model.losses(\n",
    "    references_val,\n",
    "    target_val\n",
    ")\n",
    "\n",
    "nature_test_risks = model.losses(\n",
    "    references_test,\n",
    "    target_test\n",
    ")\n",
    "\n",
    "nature_dataset_risks = model.losses(\n",
    "    references_dataset,\n",
    "    target_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define utility funcs\n",
    "def extend_param(value, length):\n",
    "    len_diff = length - value.shape[0]\n",
    "    if len_diff > 0:\n",
    "        return torch.cat((value, value[-1].expand(len_diff)))\n",
    "    else:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# get params plot\n",
    "file_format = \".pdf\"\n",
    "dpi = 144\n",
    "bbox = 'tight'\n",
    "\n",
    "max_len = 100\n",
    "\n",
    "width=None\n",
    "height=None\n",
    "\n",
    "\n",
    "base_figures_path = os.path.join(exp_paths, \"best\")\n",
    "\n",
    "\n",
    "def filename_from_title(title):\n",
    "    filename = title.replace(\"$\", \"\").replace(\"\\\\\", \"\").replace(\" \",\"_\").replace(\".\", \"\")\n",
    "    return filename + file_format\n",
    "\n",
    "params_plots = model.plot_params_over_time()\n",
    "for plot, plot_title in params_plots:\n",
    "    filename = filename_from_title(plot_title)\n",
    "    save_path = os.path.join(base_figures_path, filename)\n",
    "    plot.savefig(save_path, bbox_inches=bbox, transparent=True)\n",
    "\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"c\", \"m\"]\n",
    "plot_groups = [\n",
    "    (\"Infection Rates\", (\"alpha\", \"beta\", \"gamma\", \"delta\")),\n",
    "    (\"Detection Rates\", (\"epsilon\", \"theta\")),\n",
    "    (\"Symptoms Developing Rates\", (\"zeta\",\"eta\")),\n",
    "    (\"Threat. Symptoms Develop. Rates\", (\"mu\",\"nu\")),\n",
    "    (\"Fatality Rate\", (\"tau\",)),\n",
    "    (\"Recovery Rates\", (\"lambda\", \"kappa\", \"xi\", \"rho\", \"sigma\"))\n",
    "]\n",
    "\n",
    "\n",
    "for plot_group in plot_groups:\n",
    "    title, group_keys = plot_group\n",
    "    #print(group_keys)\n",
    "    pl_x = list(range(max_len))\n",
    "\n",
    "    curves = []\n",
    "    for param_key, color in zip(group_keys, colors):\n",
    "        param = model.extend_param(model.params[param_key], max_len)\n",
    "        param_hat_legend = f\"$\\\\{param_key}$\"\n",
    "        param_hat_curve = Curve(pl_x, param.detach().numpy(), '-', param_hat_legend, color) \n",
    "        \n",
    "        param_ref_legend = f\"{param_hat_legend} reference\"\n",
    "        param_ref_curve = Curve(pl_x, references[param_key].numpy(), '--', param_ref_legend, color) \n",
    "        curves = curves + [param_hat_curve, param_ref_curve]\n",
    "        \n",
    "    filename = filename_from_title(title)\n",
    "    save_path = os.path.join(base_figures_path, filename)\n",
    "    plot = generic_plot(curves, title, save_path, formatter=format_xtick)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "s: 46\ns: 20\ns: 16\ni: 46\ni: 20\ni: 16\nd: 46\nd: 20\nd: 16\na: 46\na: 20\na: 16\nr: 46\nr: 20\nr: 16\nt: 46\nt: 20\nt: 16\nh: 46\nh: 20\nh: 16\ne: 46\ne: 20\ne: 16\nh_detected: 46\nh_detected: 20\nh_detected: 16\nr0: <built-in method size of Tensor object at 0x0000023B846AC368>\nr0: <built-in method size of Tensor object at 0x0000023B83171598>\nr0: <built-in method size of Tensor object at 0x0000023B8325DD68>\n"
    }
   ],
   "source": [
    "# plot fits\n",
    "\n",
    "normalized_inferences = model.normalize_values(inferences, model.population)\n",
    "norm_hat_train = slice_values(normalized_inferences, train_hat_slice)\n",
    "norm_hat_val = slice_values(normalized_inferences, val_hat_slice)\n",
    "norm_hat_test = slice_values(normalized_inferences, test_hat_slice)\n",
    "\n",
    "normalized_targets = model.normalize_values(targets, model.population)\n",
    "norm_target_train = slice_values(normalized_targets, train_target_slice)\n",
    "norm_target_val = slice_values(normalized_targets, val_target_slice)\n",
    "norm_target_test = slice_values(normalized_targets, test_target_slice)\n",
    "\n",
    "\n",
    "train_range = range(0, train_size)\n",
    "val_range = range(train_size, train_size + val_size)\n",
    "test_range = range(train_size + val_size, dataset_size)\n",
    "dataset_range = range(0, dataset_size)\n",
    "\n",
    "\n",
    "for key in [\"s\", \"i\", \"d\", \"a\", \"r\", \"t\", \"h\", \"e\", \"h_detected\", \"r0\"]:\n",
    "    if key != \"r0\":\n",
    "        curr_hat_train = norm_hat_train[key]\n",
    "        curr_hat_val = norm_hat_val[key]\n",
    "        curr_hat_test = norm_hat_test[key]\n",
    "    else:\n",
    "        curr_hat_train = hat_train[key]\n",
    "        curr_hat_val = hat_val[key]\n",
    "        curr_hat_test = hat_test[key]\n",
    "\n",
    "    if key in normalized_targets:\n",
    "        target_train = norm_target_train[key]\n",
    "        target_val = norm_target_val[key]\n",
    "        target_test = norm_target_test[key]\n",
    "    else:\n",
    "        target_train = None\n",
    "        target_val = None\n",
    "        target_test = None\n",
    "\n",
    "    train_curves = model.get_curves(train_range, curr_hat_train, target_train, key, 'r')\n",
    "    # print(f\"{key}: {curr_hat_train.size}\")\n",
    "    val_curves = model.get_curves(val_range, curr_hat_val, target_val, key, 'b')\n",
    "    # print(f\"{key}: {curr_hat_val.size}\")\n",
    "    test_curves = model.get_curves(test_range, curr_hat_test, target_test, key, 'g')\n",
    "    # print(f\"{key}: {curr_hat_test.size}\")\n",
    "\n",
    "    tot_curves = train_curves + val_curves + test_curves\n",
    "\n",
    "    if references is not None:\n",
    "        reference_curve = Curve(list(dataset_range), references[key][dataset_target_slice], \"--\", label=\"Reference\")\n",
    "        tot_curves = tot_curves + [reference_curve]\n",
    "\n",
    "    pl_title = f\"{key.upper()}\"\n",
    "    filename = filename_from_title(pl_title)\n",
    "    save_path = os.path.join(base_figures_path, filename)\n",
    "    fig = generic_plot(tot_curves, pl_title, save_path, formatter=format_xtick)\n",
    "    pl_title = f\"Estimated {key.upper()} on fit\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitcovidtoolscondaed93c0ad89524ea5ad4f4946bde80980",
   "display_name": "Python 3.7.6 64-bit ('covid-tools': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}