{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from learning_models.sidarthe import Sidarthe\n",
    "from learning_models.tied_sidarthe_extended import TiedSidartheExtended\n",
    "from populations import populations\n",
    "\n",
    "from utils.data_utils import select_data\n",
    "from utils.visualization_utils import generic_plot, Curve, generic_sub_plot, Plot, generate_format_xtick\n",
    "\n",
    "from torch_euler import Heun\n",
    "import json\n",
    "\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "from utils.visualization_utils import parse_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load targets\n",
    "\n",
    "# load targets\n",
    "# df_file = os.path.join(os.getcwd(), \"data\", \"COVID-19\", \"dati-andamento-nazionale\", \"dpc-covid19-ita-andamento-nazionale.csv\")\n",
    "df_file = os.path.join(os.getcwd(), \"data\", \"dati-fr\", \"fr_data_processed.csv\")\n",
    "area = [\"FR\"]\n",
    "area_col_name = \"stato\"  # \"Country/Region\"\n",
    "\n",
    "groupby_cols = [\"data\"]  # [\"Date\"]\n",
    "\n",
    "d_col_name = \"isolamento_domiciliare\"\n",
    "r_col_name = \"ricoverati_con_sintomi\"\n",
    "t_col_name = \"terapia_intensiva\"\n",
    "h_detected_col_name = \"dimessi_guariti\"\n",
    "e_col_name = \"deceduti\"  # \"Fatalities\"\n",
    "\n",
    "x_target, d_target, dates = select_data(df_file, area, area_col_name, d_col_name, groupby_cols, file_sep=\",\")\n",
    "_, y_target, _ = select_data(df_file, area, area_col_name, \"totale_positivi\", groupby_cols, file_sep=\",\")\n",
    "_, r_target, _ = select_data(df_file, area, area_col_name, r_col_name, groupby_cols, file_sep=\",\")\n",
    "_, t_target, _ = select_data(df_file, area, area_col_name, t_col_name, groupby_cols, file_sep=\",\")\n",
    "_, h_detected_target, _ = select_data(df_file, area, area_col_name, h_detected_col_name, groupby_cols, file_sep=\",\")\n",
    "_, e_target, _ = select_data(df_file, area, area_col_name, e_col_name, groupby_cols, file_sep=\",\")\n",
    "\n",
    "initial_len = len(y_target)\n",
    "tmp_d, tmp_r, tmp_t, tmp_h, tmp_e = [], [], [], [], []\n",
    "for i in range(initial_len):\n",
    "    if d_target[i] + r_target[i] > 0:\n",
    "        tmp_d.append(d_target[i])\n",
    "        tmp_r.append(r_target[i])\n",
    "        tmp_t.append(t_target[i])\n",
    "        tmp_h.append(h_detected_target[i])\n",
    "        tmp_e.append(e_target[i])\n",
    "d_target = tmp_d\n",
    "r_target = tmp_r\n",
    "t_target = tmp_t\n",
    "h_detected_target = tmp_h\n",
    "e_target = tmp_e\n",
    "\n",
    "targets = {\n",
    "    \"d\": d_target,\n",
    "    \"r\": r_target,\n",
    "    \"t\": t_target,\n",
    "    \"h_detected\": h_detected_target,\n",
    "    \"e\": e_target\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load references\n",
    "references = {}\n",
    "param_keys = ['alpha', 'beta', 'gamma', 'delta', 'epsilon', 'theta', 'xi', 'eta', 'mu', 'nu', 'tau', 'lambda', 'kappa', 'zeta', 'rho', 'sigma']\n",
    "ref_df = pd.read_csv(os.path.join(os.getcwd(), \"data\", \"sidarthe_results_new.csv\"))\n",
    "#ref2_df = pd.read_csv(os.path.join(os.getcwd(), \"regioni\", \"sidarthe_results_2.csv\"))\n",
    "\n",
    "\n",
    "for key in 'sidarthe':\n",
    "    references[key] = ref_df[key][4:].tolist()\n",
    "\n",
    "for key in [\"h_detected\"]:\n",
    "    references[key] = ref_df[key][4:].tolist()\n",
    "\n",
    "for key in [\"r0\"]:\n",
    "    references[key] = ref_df[key][4:].tolist()\n",
    "\n",
    "for key in param_keys:\n",
    "    references[key] = ref_df[key][4:].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load experiment values\n",
    "exp_paths = os.path.join(os.getcwd())\n",
    "exp_id = \"FR_exp\"\n",
    "exp_path = os.path.join(exp_paths, exp_id)\n",
    "exp_settings_path = os.path.join(exp_path, \"settings.json\")\n",
    "exp_report_path = os.path.join(exp_path, \"final.json\")\n",
    "\n",
    "with open(exp_settings_path) as settings_json:\n",
    "    exp_settings = json.load(settings_json)\n",
    "\n",
    "with open(exp_report_path) as report_json:\n",
    "    exp_report = json.load(report_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 5.265834549137298, 'r': 12.661942437771673, 't': 72.45669439353354, 'h': 1.0, 'e': 5.705703722344078}\n"
     ]
    }
   ],
   "source": [
    "# create trained model\n",
    "population = populations[\"FR\"]\n",
    "integrator = Heun\n",
    "time_step = 1.\n",
    "\n",
    "params = exp_report[\"params\"]\n",
    "train_size = exp_settings[\"train_size\"]\n",
    "val_size = exp_settings[\"val_len\"]\n",
    "dataset_size = len(x_target)\n",
    "\n",
    "model_params = { \n",
    "    \"d_weight\": 1.,\n",
    "    \"r_weight\": 1.,\n",
    "    \"t_weight\": 1.,\n",
    "    \"h_weight\": 1.,\n",
    "    \"e_weight\": 1.,\n",
    "    \"der_1st_reg\": 0.,\n",
    "    \"bound_reg\": 0.,\n",
    "    \"verbose\": False,\n",
    "    \"loss_type\": \"rmse\",\n",
    "    \"references\": None,\n",
    "    \"targets\": targets,\n",
    "    \"train_size\": train_size,\n",
    "    \"val_size\": val_size,\n",
    "    \"first_date\": dates[0]\n",
    "}\n",
    "init_conditions_params = { \"population\": population }\n",
    "initial_conditions = Sidarthe.compute_initial_conditions_from_targets(targets, init_conditions_params)\n",
    "\n",
    "model = TiedSidartheExtended(params, population, initial_conditions, integrator, time_step, **model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute inference\n",
    "with torch.no_grad():\n",
    "    t_start = 0\n",
    "    t_end = train_size\n",
    "\n",
    "    t_grid = torch.linspace(0, dataset_size, int(dataset_size / time_step) + 1)\n",
    "    inferences = model.inference(t_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# slice dataset\n",
    "train_hat_slice = slice(t_start, int(train_size / time_step), int(1 / time_step))\n",
    "val_hat_slice = slice(int(train_size / time_step), int(train_size + val_size / time_step),int(1 / time_step))\n",
    "test_hat_slice = slice(int(train_size + val_size / time_step), int(dataset_size / time_step), int(1 / time_step))\n",
    "dataset_hat_slice = slice(t_start, int(dataset_size / time_step), int(1 / time_step))\n",
    "\n",
    "train_target_slice = slice(t_start, train_size, 1)\n",
    "val_target_slice = slice(train_size, train_size + val_size, 1)\n",
    "test_target_slice = slice(train_size + val_size, dataset_size, 1)\n",
    "dataset_target_slice = slice(t_start, dataset_size, 1)\n",
    "\n",
    "def slice_values(values, slice_):\n",
    "    return {key: value[slice_] for key, value in values.items()}\n",
    "\n",
    "hat_train = slice_values(inferences, train_hat_slice)\n",
    "hat_val = slice_values(inferences, val_hat_slice)\n",
    "hat_test = slice_values(inferences, test_hat_slice)\n",
    "hat_dataset = slice_values(inferences, dataset_hat_slice)\n",
    "\n",
    "target_train = slice_values(targets, train_target_slice)\n",
    "target_val = slice_values(targets, val_target_slice)\n",
    "target_test = slice_values(targets, test_target_slice)\n",
    "target_dataset = slice_values(targets, dataset_target_slice)\n",
    "\n",
    "\n",
    "# references = { k: torch.tensor(v, dtype=model.dtype) for k,v in references.items() }\n",
    "\n",
    "# references_train = slice_values(references, train_target_slice)\n",
    "# references_val = slice_values(references, val_target_slice)\n",
    "# references_test = slice_values(references, test_target_slice)\n",
    "# references_dataset = slice_values(references, dataset_target_slice)\n",
    "\n",
    "# normalized_references = model.normalize_values(targets, 1./model.population)\n",
    "# norm_references_train = slice_values(normalized_references, train_hat_slice)\n",
    "# norm_references_val = slice_values(normalized_references, val_hat_slice)\n",
    "# norm_references_test = slice_values(normalized_references, test_hat_slice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define utility funcs\n",
    "def extend_param(value, length):\n",
    "    len_diff = length - value.shape[0]\n",
    "    if len_diff > 0:\n",
    "        return torch.cat((value, value[-1].expand(len_diff)))\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_slice = slice(0, train_size)\n",
    "val_slice = slice(train_size, train_size + val_size)\n",
    "test_slice = slice(train_size + val_size, dataset_size)\n",
    "\n",
    "slices = {\n",
    "    \"train\": train_slice,\n",
    "    \"val\": val_slice,\n",
    "    \"test\": test_slice\n",
    "}\n",
    "\n",
    "dataset_slice = slice(0, dataset_size)\n",
    "\n",
    "test_size = dataset_size - train_size - val_size\n",
    "\n",
    "def get_column(values, col_slice):\n",
    "\n",
    "    column = [None] * col_slice.start + values + [None] * (dataset_size - col_slice.stop)\n",
    "    return column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "figs_path = os.path.join(exp_path, \"figs\")\n",
    "if not os.path.exists(figs_path):\n",
    "    os.makedirs(figs_path)\n",
    "\n",
    "def savefig(figure, figures_path, figure_name):\n",
    "    fig_path = os.path.join(figures_path, f\"{figure_name}.pdf\")\n",
    "    figure.savefig(fig_path, bbox_inches='tight', transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_dfs = {}\n",
    "params = model.params\n",
    "start_date = parse_date(dates[0])\n",
    "dates_x = [(start_date + datetime.timedelta(int(n))).strftime(\"%Y-%m-%d\") for n in range(0, dataset_size)]\n",
    "sns.set()\n",
    "\n",
    "\n",
    "for group_name, group in model.param_groups.items():\n",
    "    group_df = pd.DataFrame()\n",
    "    group_df['date'] = pd.to_datetime(dates_x)\n",
    "    group_df.set_index('date', inplace=True)\n",
    "    #group_df['test'] = [i for i in range(0, len(dates_x))]\n",
    "    for param_name in group:\n",
    "        group_df[f\"$\\{param_name}$\"] = extend_param(params[param_name], dataset_size).detach().numpy()\n",
    "\n",
    "    plot_title = group_name.replace(\"_\", \" \").title()\n",
    "    plot = group_df.plot(title=plot_title, figsize=(6,4))\n",
    "    figure = plot.get_figure()\n",
    "    savefig(figure, figs_path, group_name)\n",
    "\n",
    "    #group_dfs[group_name] = group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values does not match length of index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-04a2e3c5d6c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mcurve_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"Prediction ({section})\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msection_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mc_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mcurve_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf\"Target ({section})\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_targets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msection_slice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m             \u001b[0mcurve_styles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mcurve_colors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'g'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\multipoetgen\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2937\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2938\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\multipoetgen\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   2998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3000\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\multipoetgen\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3635\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3636\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3637\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3638\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\multipoetgen\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index, copy)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of values does not match length of index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values does not match length of index"
     ]
    }
   ],
   "source": [
    "for key in [\"s\", \"i\", \"d\", \"a\", \"r\", \"t\", \"h\", \"e\", \"h_detected\", \"r0\"]:\n",
    "#for key in [\"d\"]:\n",
    "\n",
    "    plot_title = f\"{key.upper()} - train/validation/test\"\n",
    "\n",
    "    hats = {\n",
    "        \"train\": hat_train[key].numpy().tolist(),\n",
    "        \"val\": hat_val[key].numpy().tolist(),\n",
    "        \"test\": hat_test[key].numpy().tolist()\n",
    "    }\n",
    "\n",
    "    if key in targets:\n",
    "        c_targets = {\n",
    "            \"train\": target_train[key],\n",
    "            \"val\": target_val[key],\n",
    "            \"test\": target_test[key]\n",
    "        }\n",
    "    else:\n",
    "        c_targets = {\n",
    "            \"train\": None,\n",
    "            \"val\": None,\n",
    "            \"test\": None\n",
    "        }\n",
    "\n",
    "    curve_df = pd.DataFrame()\n",
    "    curve_df['date'] = pd.to_datetime(dates_x[:])\n",
    "    curve_df.set_index('date', inplace=True)\n",
    "\n",
    "\n",
    "    for section, section_slice in slices.items():\n",
    "        curve_df[f\"Prediction ({section})\"] = get_column(hats[section], section_slice)\n",
    "        if c_targets[section] is not None: \n",
    "            curve_df[f\"Target ({section})\"] = get_column(c_targets[section], section_slice)\n",
    "            curve_styles = ['-', '.'] * 3\n",
    "            curve_colors = ['r'] * 2 + ['b'] * 2 + ['g'] * 2\n",
    "        else:\n",
    "            curve_styles = ['-'] * 3\n",
    "            curve_colors = ['r', 'b', 'g']\n",
    "\n",
    "\n",
    "    plot = curve_df.plot(title=plot_title, style=curve_styles, color=curve_colors, figsize=(10,6), grid=True)\n",
    "    figure = plot.get_figure()\n",
    "    savefig(figure, figs_path, key)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:multipoetgen]",
   "language": "python",
   "name": "conda-env-multipoetgen-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
